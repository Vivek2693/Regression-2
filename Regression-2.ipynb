{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfc2354f",
   "metadata": {},
   "source": [
    "### Q1. R-Squared in Linear Regression:\n",
    "\n",
    "Concept: R-sQuared (R¬≤) is a statistical measure that represents the proportion of the variance in the dependent variable that is explained by the independent variables in a linear regression model.\n",
    "Calculation: R-sQuared is calculated as the ratio of the explained sum of s### Quares (ESS) to the total sum of s### Quares (TSS). Mathematically, it is expressed as:\n",
    "The coefficient of determination (R¬≤) is calculated as: $R^2 = 1 - \\frac{RSS}{TSS}$\n",
    "\n",
    " where RSS is the residual sum of sQuares (unexplained variance) and TSS is the total sum of s### Quares.\n",
    "Interpretation: R-s### Quared ranges from 0 to 1, where 0 indicates that the model explains none of the variability of the response data around its mean, and 1 indicates that the model explains all the variability of the response data around its mean.\n",
    "### Q2. Adjusted R-sQuared:\n",
    "\n",
    "Definition: Adjusted R-sQuared is a modified version of R-Quared that adjusts for the number of predictors (independent variables) in the model.\n",
    "Calculation: $$R^2 = 1 - \\left( 1 - \\frac{RSS}{TSS} \\right) \\frac{n - 1}{n - p - 1}$$\n",
    "\n",
    "**Adjusted R-squared:**\n",
    "\n",
    "$$R^2_{adj} = 1 - \\frac{n - p - 1}{n - 1} (1 - R^2)$$\n",
    "\n",
    "where:\n",
    "\n",
    "* n is the number of observations.\n",
    "* p is the number of predictors in the model.\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "R-squared is a commonly used metric to assess the goodness-of-fit of a regression model. However, it can be misleading when adding more predictors, even if they are not statistically significant. Adjusted R-squared addresses this limitation by penalizing the addition of unnecessary predictors in the model.\n",
    "\n",
    "As you can see, the adjusted R-squared formula includes the number of observations (n) and predictors (p) to account for the model's complexity. It will typically be lower than R-squared, providing a more accurate measure of the model's performance when considering the number of variables used.\n",
    "\n",
    " where \n",
    "ùëõ\n",
    "n is the number of observations and \n",
    "ùëù\n",
    "p is the number of predictors in the model.\n",
    "Difference: Adjusted R-sQuared penalizes the addition of unnecessary predictors in the model, unlike R-s### Quared, which may increase even with the addition of irrelevant predictors.\n",
    "### Q3. Appropriate Use of Adjusted R-sQuared:\n",
    "\n",
    "Adjusted R-sQuared is more appropriate when comparing models with different numbers of predictors or when determining the goodness of fit of a model with many predictors.\n",
    "### Q4. RMSE, MSE, and MAE:\n",
    "\n",
    "RMSE (Root Mean SQuared Error): RMSE is the sQuare root of the average of the sQuared differences between predicted and actual values. Mathematically, it is calculated as:\n",
    "$$RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}$$\n",
    "\n",
    "**MSE (Mean Squared Error):**\n",
    "\n",
    "MSE is the average of the squared differences between predicted and actual values. It is calculated as:\n",
    "\n",
    "$$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "**MAE (Mean Absolute Error):**\n",
    "\n",
    "MAE is the average of the absolute differences between predicted and actual values. It is calculated as:\n",
    "\n",
    "$$MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|$$\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* `RMSE`: Root Mean Squared Error.\n",
    "* `MSE`: Mean Squared Error.\n",
    "* `MAE`: Mean Absolute Error.\n",
    "* `n`: Number of observations.\n",
    "* `y_i`: Actual value for observation i.\n",
    "* `≈∑_i`: Predicted value for observation i.\n",
    "\n",
    "RMSE squares the errors before averaging, giving more weight to larger errors. MAE uses absolute differences, making it less sensitive to outliers compared to RMSE. Both metrics are used to evaluate the performance of regression models.\n",
    "\n",
    "### Q5. Advantages and Disadvantages of RMSE, MSE, and MAE:\n",
    "\n",
    "Advantages: RMSE, MSE, and MAE provide ### Quantitative measures of the error between predicted and actual values, allowing for easy comparison between models. RMSE penalizes large errors more than MSE and MAE, which may be desirable in some scenarios.\n",
    "Disadvantages: RMSE, MSE, and MAE do not provide insight into the direction of errors and treat overestimation and underestimation e### Qually. RMSE and MSE are sensitive to outliers, while MAE is less sensitive. Choosing the appropriate metric depends on the specific characteristics of the problem and the desired properties of the evaluation metric.\n",
    "### Q6. Lasso Regularization:\n",
    "\n",
    "Concept: Lasso regularization (L1 regularization) adds a penalty term to the linear regression objective function, which is the absolute value of the coefficients multiplied by a regularization parameter (lambda).\n",
    "Difference from Ridge Regularization: Lasso regularization encourages sparsity in the coefficient values, leading to some coefficients being exactly zero, effectively performing feature selection. In contrast, Ridge regularization (L2 regularization) penalizes the s### Quared magnitude of coefficients without necessarily setting any coefficients to zero.\n",
    "Appropriate Use: Lasso regularization is more appropriate when the dataset contains many irrelevant or redundant features that can be eliminated from the model.\n",
    "### Q7. Role of Regularized Linear Models in Preventing Overfitting:\n",
    "\n",
    "Regularized linear models add penalty terms to the objective function, which penalize large coefficient values. This prevents the model from becoming overly complex and reduces the risk of overfitting by discouraging high variance.\n",
    "For example, in Lasso regularization, the penalty term encourages sparsity in the coefficient values, effectively performing feature selection and preventing the model from fitting noise in the data.\n",
    "### Q8. Limitations of Regularized Linear Models:\n",
    "\n",
    "Regularized linear models may not perform well when there is insufficient regularization, leading to underfitting, or when there is too much regularization, leading to oversimplified models.\n",
    "Additionally, regularized linear models may not capture complex non-linear relationships between variables as effectively as other methods such as tree-based models or neural networks.\n",
    "### Q9. Comparison of Regression Models Based on Evaluation Metrics:\n",
    "\n",
    "Model A has an RMSE of 10, while Model B has an MAE of 8. In this scenario, we would choose Model B as the better performer because it has a lower MAE, indicating that it has smaller errors on average compared to Model A.\n",
    "However, it's essential to consider the specific characteristics of the problem and the limitations of each evaluation metric. RMSE penalizes large errors more heavily than MAE, so if the dataset contains outliers, RMSE may be more sensitive to them than MAE.\n",
    "### Q10. Comparison of Regularized Linear Models Based on Regularization:\n",
    "\n",
    "Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5.\n",
    "The choice between the two models depends on the specific characteristics of the dataset and the desired properties of the regularization method. If feature selection is desired, Lasso regularization may be preferred due to its ability to set some coefficients to zero. However, if multicollinearity is a concern, Ridge regularization may be more appropriate as it does not eliminate features entirely.\n",
    "It's essential to consider the trade-offs and limitations of each regularization method and choose the one that best suits the problem at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64078aad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
